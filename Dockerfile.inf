FROM nvidia/cuda:13.0.2-devel-ubuntu24.04 AS builder

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    cmake \
    curl \
    libcurl4-openssl-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /build

RUN git clone https://github.com/ggerganov/llama.cpp.git .

RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 \
    && echo "/usr/local/cuda/lib64/stubs" > /etc/ld.so.conf.d/cuda-stubs.conf \
    && ldconfig \
    && cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_NATIVE=ON \
    -DCMAKE_CUDA_ARCHITECTURES=120 \
    -DCMAKE_BUILD_TYPE=Release \
    && cmake --build build --config Release -j$(nproc)

FROM nvidia/cuda:13.0.2-runtime-ubuntu24.04

COPY --from=builder /build/build/bin/llama-server /app/llama-server
COPY --from=builder /build/build/bin/llama-cli /app/llama-cli
COPY --from=builder /build/build/bin/*.so* /app/

COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /usr/local/bin/

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-venv \
    curl \
    ffmpeg \
    libgomp1 \
    espeak-ng \
    libsndfile1 \
    git \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY pyproject.toml ./
COPY scripts/ ./scripts/

RUN uv venv /app/.venv
RUN uv sync --no-dev --extra inference --no-install-project

COPY src/ ./src/
RUN uv sync --no-dev --extra inference
RUN uv pip install pip
RUN uv cache clean

ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PATH="/app/.venv/bin:$PATH" \
    UV_CACHE_DIR=/tmp/uv-cache \
    LD_LIBRARY_PATH="/app:$LD_LIBRARY_PATH"


EXPOSE 8001 8051

ENTRYPOINT []

CMD ["inf_tts"]
